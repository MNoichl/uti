{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bayesian-optimization\n",
    "# !pip install iteration-utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for development:\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# standards:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# graphics:\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from cmcrameri import cm\n",
    "import cmocean\n",
    "\n",
    "import tqdm\n",
    "# Fix global random state\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pl\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import dill as pickle      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir instep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8, 2, 2, 2, 3)\n",
      "[array([[[[[1.08 , 1.08 , 1.08 ],\n",
      "          [1.08 , 1.08 , 1.08 ]],\n",
      "\n",
      "         [[1.08 , 1.08 , 1.08 ],\n",
      "          [1.08 , 1.08 , 1.08 ]]],\n",
      "\n",
      "\n",
      "        [[[0.712, 0.712, 0.712],\n",
      "          [0.712, 0.712, 0.712]],\n",
      "\n",
      "         [[0.712, 0.712, 0.712],\n",
      "          [0.712, 0.712, 0.712]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.528, 0.528, 0.528],\n",
      "          [0.528, 0.528, 0.528]],\n",
      "\n",
      "         [[0.528, 0.528, 0.528],\n",
      "          [0.528, 0.528, 0.528]]],\n",
      "\n",
      "\n",
      "        [[[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]],\n",
      "\n",
      "         [[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]],\n",
      "\n",
      "         [[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]]],\n",
      "\n",
      "\n",
      "        [[[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]],\n",
      "\n",
      "         [[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.24 , 0.24 , 0.24 ],\n",
      "          [0.24 , 0.24 , 0.24 ]],\n",
      "\n",
      "         [[0.24 , 0.24 , 0.24 ],\n",
      "          [0.24 , 0.24 , 0.24 ]]],\n",
      "\n",
      "\n",
      "        [[[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]],\n",
      "\n",
      "         [[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]],\n",
      "\n",
      "         [[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]]],\n",
      "\n",
      "\n",
      "        [[[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]],\n",
      "\n",
      "         [[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.24 , 0.24 , 0.24 ],\n",
      "          [0.24 , 0.24 , 0.24 ]],\n",
      "\n",
      "         [[0.24 , 0.24 , 0.24 ],\n",
      "          [0.24 , 0.24 , 0.24 ]]],\n",
      "\n",
      "\n",
      "        [[[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]],\n",
      "\n",
      "         [[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]],\n",
      "\n",
      "         [[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]]],\n",
      "\n",
      "\n",
      "        [[[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]],\n",
      "\n",
      "         [[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.6  , 0.6  , 0.6  ],\n",
      "          [0.6  , 0.6  , 0.6  ]],\n",
      "\n",
      "         [[0.6  , 0.6  , 0.6  ],\n",
      "          [0.6  , 0.6  , 0.6  ]]],\n",
      "\n",
      "\n",
      "        [[[1.   , 1.   , 1.   ],\n",
      "          [1.   , 1.   , 1.   ]],\n",
      "\n",
      "         [[1.   , 1.   , 1.   ],\n",
      "          [1.   , 1.   , 1.   ]]]]]), array([[[[[1.08 , 1.08 , 1.08 ],\n",
      "          [1.08 , 1.08 , 1.08 ]],\n",
      "\n",
      "         [[1.08 , 1.08 , 1.08 ],\n",
      "          [1.08 , 1.08 , 1.08 ]]],\n",
      "\n",
      "\n",
      "        [[[0.712, 0.712, 0.712],\n",
      "          [0.712, 0.712, 0.712]],\n",
      "\n",
      "         [[0.712, 0.712, 0.712],\n",
      "          [0.712, 0.712, 0.712]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.528, 0.528, 0.528],\n",
      "          [0.528, 0.528, 0.528]],\n",
      "\n",
      "         [[0.528, 0.528, 0.528],\n",
      "          [0.528, 0.528, 0.528]]],\n",
      "\n",
      "\n",
      "        [[[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]],\n",
      "\n",
      "         [[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]],\n",
      "\n",
      "         [[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]]],\n",
      "\n",
      "\n",
      "        [[[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]],\n",
      "\n",
      "         [[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.24 , 0.24 , 0.24 ],\n",
      "          [0.24 , 0.24 , 0.24 ]],\n",
      "\n",
      "         [[0.24 , 0.24 , 0.24 ],\n",
      "          [0.24 , 0.24 , 0.24 ]]],\n",
      "\n",
      "\n",
      "        [[[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]],\n",
      "\n",
      "         [[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]],\n",
      "\n",
      "         [[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]]],\n",
      "\n",
      "\n",
      "        [[[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]],\n",
      "\n",
      "         [[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.24 , 0.24 , 0.24 ],\n",
      "          [0.24 , 0.24 , 0.24 ]],\n",
      "\n",
      "         [[0.24 , 0.24 , 0.24 ],\n",
      "          [0.24 , 0.24 , 0.24 ]]],\n",
      "\n",
      "\n",
      "        [[[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]],\n",
      "\n",
      "         [[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]],\n",
      "\n",
      "         [[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]]],\n",
      "\n",
      "\n",
      "        [[[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]],\n",
      "\n",
      "         [[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.6  , 0.6  , 0.6  ],\n",
      "          [0.6  , 0.6  , 0.6  ]],\n",
      "\n",
      "         [[0.6  , 0.6  , 0.6  ],\n",
      "          [0.6  , 0.6  , 0.6  ]]],\n",
      "\n",
      "\n",
      "        [[[1.   , 1.   , 1.   ],\n",
      "          [1.   , 1.   , 1.   ]],\n",
      "\n",
      "         [[1.   , 1.   , 1.   ],\n",
      "          [1.   , 1.   , 1.   ]]]]]), array([[[[[1.08 , 1.08 , 1.08 ],\n",
      "          [1.08 , 1.08 , 1.08 ]],\n",
      "\n",
      "         [[1.08 , 1.08 , 1.08 ],\n",
      "          [1.08 , 1.08 , 1.08 ]]],\n",
      "\n",
      "\n",
      "        [[[0.712, 0.712, 0.712],\n",
      "          [0.712, 0.712, 0.712]],\n",
      "\n",
      "         [[0.712, 0.712, 0.712],\n",
      "          [0.712, 0.712, 0.712]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.528, 0.528, 0.528],\n",
      "          [0.528, 0.528, 0.528]],\n",
      "\n",
      "         [[0.528, 0.528, 0.528],\n",
      "          [0.528, 0.528, 0.528]]],\n",
      "\n",
      "\n",
      "        [[[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]],\n",
      "\n",
      "         [[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]],\n",
      "\n",
      "         [[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]]],\n",
      "\n",
      "\n",
      "        [[[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]],\n",
      "\n",
      "         [[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.24 , 0.24 , 0.24 ],\n",
      "          [0.24 , 0.24 , 0.24 ]],\n",
      "\n",
      "         [[0.24 , 0.24 , 0.24 ],\n",
      "          [0.24 , 0.24 , 0.24 ]]],\n",
      "\n",
      "\n",
      "        [[[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]],\n",
      "\n",
      "         [[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]],\n",
      "\n",
      "         [[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]]],\n",
      "\n",
      "\n",
      "        [[[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]],\n",
      "\n",
      "         [[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.24 , 0.24 , 0.24 ],\n",
      "          [0.24 , 0.24 , 0.24 ]],\n",
      "\n",
      "         [[0.24 , 0.24 , 0.24 ],\n",
      "          [0.24 , 0.24 , 0.24 ]]],\n",
      "\n",
      "\n",
      "        [[[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]],\n",
      "\n",
      "         [[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]],\n",
      "\n",
      "         [[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]]],\n",
      "\n",
      "\n",
      "        [[[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]],\n",
      "\n",
      "         [[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.6  , 0.6  , 0.6  ],\n",
      "          [0.6  , 0.6  , 0.6  ]],\n",
      "\n",
      "         [[0.6  , 0.6  , 0.6  ],\n",
      "          [0.6  , 0.6  , 0.6  ]]],\n",
      "\n",
      "\n",
      "        [[[1.   , 1.   , 1.   ],\n",
      "          [1.   , 1.   , 1.   ]],\n",
      "\n",
      "         [[1.   , 1.   , 1.   ],\n",
      "          [1.   , 1.   , 1.   ]]]]]), array([[[[[1.08 , 1.08 , 1.08 ],\n",
      "          [1.08 , 1.08 , 1.08 ]],\n",
      "\n",
      "         [[1.08 , 1.08 , 1.08 ],\n",
      "          [1.08 , 1.08 , 1.08 ]]],\n",
      "\n",
      "\n",
      "        [[[0.712, 0.712, 0.712],\n",
      "          [0.712, 0.712, 0.712]],\n",
      "\n",
      "         [[0.712, 0.712, 0.712],\n",
      "          [0.712, 0.712, 0.712]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.528, 0.528, 0.528],\n",
      "          [0.528, 0.528, 0.528]],\n",
      "\n",
      "         [[0.528, 0.528, 0.528],\n",
      "          [0.528, 0.528, 0.528]]],\n",
      "\n",
      "\n",
      "        [[[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]],\n",
      "\n",
      "         [[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]],\n",
      "\n",
      "         [[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]]],\n",
      "\n",
      "\n",
      "        [[[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]],\n",
      "\n",
      "         [[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.24 , 0.24 , 0.24 ],\n",
      "          [0.24 , 0.24 , 0.24 ]],\n",
      "\n",
      "         [[0.24 , 0.24 , 0.24 ],\n",
      "          [0.24 , 0.24 , 0.24 ]]],\n",
      "\n",
      "\n",
      "        [[[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]],\n",
      "\n",
      "         [[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]],\n",
      "\n",
      "         [[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]]],\n",
      "\n",
      "\n",
      "        [[[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]],\n",
      "\n",
      "         [[0.16 , 0.16 , 0.16 ],\n",
      "          [0.16 , 0.16 , 0.16 ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.24 , 0.24 , 0.24 ],\n",
      "          [0.24 , 0.24 , 0.24 ]],\n",
      "\n",
      "         [[0.24 , 0.24 , 0.24 ],\n",
      "          [0.24 , 0.24 , 0.24 ]]],\n",
      "\n",
      "\n",
      "        [[[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]],\n",
      "\n",
      "         [[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]],\n",
      "\n",
      "         [[0.   , 0.   , 0.   ],\n",
      "          [0.   , 0.   , 0.   ]]],\n",
      "\n",
      "\n",
      "        [[[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]],\n",
      "\n",
      "         [[0.4  , 0.4  , 0.4  ],\n",
      "          [0.4  , 0.4  , 0.4  ]]]],\n",
      "\n",
      "\n",
      "\n",
      "       [[[[0.6  , 0.6  , 0.6  ],\n",
      "          [0.6  , 0.6  , 0.6  ]],\n",
      "\n",
      "         [[0.6  , 0.6  , 0.6  ],\n",
      "          [0.6  , 0.6  , 0.6  ]]],\n",
      "\n",
      "\n",
      "        [[[1.   , 1.   , 1.   ],\n",
      "          [1.   , 1.   , 1.   ]],\n",
      "\n",
      "         [[1.   , 1.   , 1.   ],\n",
      "          [1.   , 1.   , 1.   ]]]]])]\n",
      "1.0799999999999998\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Number of strategies is not equal to the number of players'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r0/lw7swd9j28j6k6x4rlrqrtjr0000gn/T/ipykernel_51514/1478431118.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhigh_mid_low\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteam_reasoning_n_agents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthree_agent_hilo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/PhD/Programming/MesaTutorial/uti/simple_teams/n_agents_reasoning.py\u001b[0m in \u001b[0;36mteam_reasoning_n_agents\u001b[0;34m(m, n, game, omega)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndenumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpayoffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# iy is a n+1 tuple indicating the played strategies, values are the utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"current_game.nfg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpygambit/lib/game.pxi\u001b[0m in \u001b[0;36mpygambit.lib.libgambit.Game.__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Number of strategies is not equal to the number of players'"
     ]
    }
   ],
   "source": [
    "from simple_teams.n_agents_reasoning import team_reasoning_n_agents\n",
    "\n",
    "three_agent_hilo = np.array([[[[3,3,3], [0,0,0]], [[0,0,0], [0,0,0]]], [[[0,0,0], [0,0,0]],[[0,0,0], [1,1,1]]]])\n",
    "\n",
    "high_mid_low = np.array([[[[3,3,3], [0,0,0], [0,0,0]], [[0,0,0],[0,0,0], [0,0,0]], [[0,0,0], [0,0,0], [0,0,0]]], [[[0,0,0], [0,0,0], [0,0,0]], [[0,0,0],[2,2,2], [0,0,0]], [[0,0,0], [0,0,0], [0,0,0]]], [[[0,0,0], [0,0,0], [0,0,0]], [[0,0,0],[0,0,0], [0,0,0]], [[0,0,0], [0,0,0], [1,1,1]]]])\n",
    "\n",
    "print(team_reasoning_n_agents(2, 3, three_agent_hilo, 0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e13016e8e1d4678ab9dce9910b1eb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SankeyWidget(layout=Layout(height='170', width='700'), links=[{'source': '(0) init', 'target': '(1) Player: 1,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n",
      "[(0, 0), (0, 1), (1, 0), (1, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n",
      "Search for Nash equilibria in pure strategies\n",
      "Gambit version 16.0.2, Copyright (C) 1994-2022, The Gambit Project\n",
      "This is free software, distributed under the GNU GPL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from simple_teams.game_structure import game, game_mixture\n",
    "\n",
    "prisoners_dilemma = game(\n",
    "    name=\"Prisoners Dilemma\",\n",
    "    n_players=2,\n",
    "    n_choices=2,\n",
    "    payoffs=[[3, 3], [-3, 4],[4, -3],  [1,1]],\n",
    "#     payoffs=[[2, 3], [0, -6],[0, 2],  [1,1]],\n",
    "\n",
    "    size=dict(width=700, height=170),\n",
    ")\n",
    "\n",
    "prisoners_dilemma.show_game()\n",
    "prisoners_dilemma.set_up_TR_strategies(100)\n",
    "# prisoners_dilemma.plot_TR_utils(player=0,figsize=(13, 5))\n",
    "# plt.savefig('instep/pd_utils.png',dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hilo = game(\n",
    "    name=\"HiLo\",\n",
    "    n_players=2,\n",
    "    n_choices=2,\n",
    "    payoffs=[[3,3], [0,0], [0,0], [1,1]],\n",
    "#     payoffs=[[2,1], [0,0], [0,0], [1,2]],\n",
    "\n",
    "    size=dict(width=700, height=170),\n",
    ")\n",
    "\n",
    "hilo.show_game()\n",
    "\n",
    "hilo.set_up_TR_strategies(100)\n",
    "hilo.plot_TR_utils(player=1,figsize=(13, 5))\n",
    "# plt.savefig('instep/hilo_utils.png',dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hilo.player_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "my_game_mixture = game_mixture([[prisoners_dilemma,.5],\n",
    "                                [hilo,0.5]])\n",
    "\n",
    "\n",
    "\n",
    "from simple_teams.team_reasoning import calculate_utils\n",
    "\n",
    "matrix_player_0 = game.return_players_matrix(prisoners_dilemma, 0)\n",
    "\n",
    "print(matrix_player_0)\n",
    "\n",
    "print(calculate_utils(0.5, matrix_player_0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G = nx.random_partition_graph([8 for x in range(4)], 0.6,0.01)\n",
    "G = nx.complete_graph(32)\n",
    "# G = nx.powerlaw_cluster_graph(32,1,0.01)\n",
    "\n",
    "# G =  nx.erdos_renyi_graph(32,p=0.4)\n",
    "def ensure_graph_connectivity(G):\n",
    "    for node in list(nx.isolates(G)):\n",
    "        random_connection = node\n",
    "        while random_connection == node:\n",
    "            random_connection = np.random.randint(len(G.nodes()))\n",
    "        G.add_edge(node,random_connection)\n",
    "    return G\n",
    "\n",
    "def plot_graph(G, figsize=(10, 10)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plt.style.use(\"opinions.mplstyle\")\n",
    "    positions = nx.kamada_kawai_layout(G)\n",
    "    nx.draw_networkx_nodes(G, positions, node_size=120, node_color=\"#1a2340\", alpha=0.8)\n",
    "    nx.draw_networkx_edges(G, positions, edge_color=\"grey\", width=1, alpha=1)\n",
    "    plt.axis('off')\n",
    "\n",
    "G = ensure_graph_connectivity(G)\n",
    "plot_graph(G)\n",
    "plt.savefig('instep/complete_graph_img.png',dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_teams import model \n",
    "from simple_teams.learners import bayesian_gaussian_process \n",
    "\n",
    "model = model.team_reasoning_model(\n",
    "    proportion_team_reasoners=0.7,\n",
    "    n_agents=32,\n",
    "    init_network=G,\n",
    "    games=my_game_mixture,#prisoners_dilemma,#my_game_mixture,\n",
    "    probability_team_reasoning=1,\n",
    "    utility_calculation=\"expected_utility\",\n",
    "    learner = bayesian_gaussian_process(window=12, kappa=4, xi=0.0, alpha=.7)\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "# expected_utility\n",
    "for t in tqdm.tqdm_notebook(range(40)):\n",
    "    model.step()\n",
    "    \n",
    "    \n",
    "# https://www.youtube.com/watch?v=aZa8Wd8Nc8o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "plt.plot([x['omega'] for x  in model.schedule._agents[k].my_learner.attempts])\n",
    "plt.show()\n",
    "\n",
    "for k in range(0,len(model.schedule._agents)):\n",
    "    print(model.schedule._agents[k].team_reasoner)\n",
    "    if model.schedule._agents[k].team_reasoner:\n",
    "        c = 'green'\n",
    "    else:\n",
    "        c ='red'\n",
    "#     print(model.schedule._agents[k].team_reasoner)\n",
    "    plt.plot(np.cumsum([x['target'] for x  in model.schedule._agents[k].my_learner.attempts]),c=c)\n",
    "plt.show()\n",
    "\n",
    "# model.schedule._agents[k].my_learner.attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "step = 38\n",
    "omega = [x['omega'] for x  in model.schedule._agents[k].my_learner.attempts]\n",
    "target = [x['target'] for x  in model.schedule._agents[k].my_learner.attempts]\n",
    "dist_from_point = np.abs(step-np.linspace(0,len(omega),len(omega)))\n",
    "print(dist_from_point)\n",
    "\n",
    "cmap = cmocean.cm.solar_r  # pl.cm.RdBu\n",
    "my_cmap = cmap(np.arange(cmap.N))\n",
    "alphas = np.linspace(0, 1, cmap.N)\n",
    "my_cmap[:, -1] = alphas\n",
    "my_tr_cmap = ListedColormap(my_cmap)\n",
    "\n",
    "\n",
    "plt.scatter(omega,target,c=dist_from_point,cmap=my_tr_cmap)\n",
    "\n",
    "distribution = model.schedule._agents[k].my_learner.distributions[step]\n",
    "x = np.linspace(0, 1, 100)\n",
    "# print(distribution)\n",
    "plt.fill_between(x, distribution[0] + distribution[1], distribution[0] - distribution[1], alpha=0.1)\n",
    "\n",
    "plt.plot(x,distribution[0], c=\"#1a2340\", alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"Omega\", labelpad=10)\n",
    "plt.ylabel(\"Received Payoff\", labelpad=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "ax = axes3d.Axes3D(fig)\n",
    "ax.view_init(10, 50)\n",
    "\n",
    "import cmocean\n",
    "\n",
    "for ix, distribution in enumerate(\n",
    "    model.schedule._agents[9].my_learner.distributions\n",
    "):\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    plt.plot([ix] * len(x), x, zs=distribution[0], c=\"#1a2340\", alpha=0.5)\n",
    "#     expected_value = distribution.expect()\n",
    "#     ax.scatter(\n",
    "#         np.array(ix),\n",
    "#         np.array(expected_value),\n",
    "#         np.array(distribution.pdf(expected_value)),\n",
    "#         s=10,\n",
    "#         c=\"black\", alpha=0.6,\n",
    "#     )\n",
    "plt.title(\"Learning-progress of an individual agent\", y=0.87)\n",
    "ax.invert_xaxis()\n",
    "\n",
    "plt.xlabel(\"Timestep\", labelpad=10)\n",
    "plt.ylabel(\"Probabilities\", labelpad=10)\n",
    "ax.set_zlabel(\"Probability-density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pl\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmap = cmocean.cm.solar_r  # pl.cm.RdBu\n",
    "my_cmap = cmap(np.arange(cmap.N))\n",
    "alphas = np.linspace(0, 1, cmap.N)\n",
    "my_cmap[:, -1] = alphas\n",
    "my_tr_cmap = ListedColormap(my_cmap)\n",
    "\n",
    "cmap = cmocean.cm.deep  # pl.cm.RdBu\n",
    "my_cmap = cmap(np.arange(cmap.N))\n",
    "alphas = np.linspace(0, 1, cmap.N)\n",
    "my_cmap[:, -1] = alphas\n",
    "my_non_tr_cmap = ListedColormap(my_cmap)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "agents = [model.schedule._agents[x] for x in model.schedule._agents.keys()]\n",
    "# agents = agents[0:2]\n",
    "mappings = []\n",
    "for agent in tqdm.tqdm_notebook(agents):\n",
    "    collected_values = []\n",
    "    for ix, distribution in enumerate(agent.my_learner.distributions):\n",
    "\n",
    "        n = 100\n",
    "        x = np.linspace(0, 1, n)\n",
    "        alphas = distribution[0]#distribution.pdf(x)\n",
    "        #         alphas = (alphas - np.min(alphas)) / (np.max(alphas) - np.min(alphas))\n",
    "        collected_values.append(alphas)\n",
    "    mappings.append(np.hstack([x.reshape(-1, 1) for x in collected_values]))\n",
    "\n",
    "\n",
    "sns.heatmap(\n",
    "np.mean(\n",
    "    np.stack(\n",
    "        [\n",
    "            mappings[x]\n",
    "            for x in np.where([agent.team_reasoner for agent in agents])[0]\n",
    "        ], axis=2\n",
    "    ),\n",
    "    axis=2,\n",
    "),\n",
    "    cmap=my_tr_cmap,\n",
    "    cbar=True,\n",
    ")\n",
    "ax.invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "sns.heatmap(\n",
    "np.mean(\n",
    "    np.stack(\n",
    "        [\n",
    "            mappings[x]\n",
    "            for x in np.where([not agent.team_reasoner for agent in agents])[0]\n",
    "        ], axis=2\n",
    "    ),\n",
    "    axis=2,\n",
    "),\n",
    "    \n",
    "    cmap=my_non_tr_cmap,\n",
    "    cbar=True,\n",
    ")\n",
    "ax.invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multirun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from simple_teams import model \n",
    "from simple_teams.learners import bayesian_gaussian_process \n",
    "\n",
    "\n",
    "model_list = []\n",
    "\n",
    "\n",
    "\n",
    "pd_share = [0.5]#np.linspace(0,1,9)#9\n",
    "tr_proportions = np.linspace(0,1,21)#11\n",
    "\n",
    "for this_pd_share in tqdm.tqdm_notebook(pd_share):\n",
    "    my_game_mixture = game_mixture([[prisoners_dilemma,this_pd_share],\n",
    "                                [hilo,1-this_pd_share]])\n",
    "    \n",
    "    for tr_proportion in tqdm.tqdm_notebook(tr_proportions):\n",
    "        n=0\n",
    "        while n<3: #dirty hack to avoid untracked error\n",
    "#         for ix in range(0,3):\n",
    "    #         G = nx.powerlaw_cluster_graph(32,1,0.01)\n",
    "            try:\n",
    "                G = nx.random_partition_graph([8 for x in range(4)], 0.6,0.01)\n",
    "\n",
    "                G = ensure_graph_connectivity(G)\n",
    "\n",
    "\n",
    "                this_model = model.team_reasoning_model(\n",
    "                    proportion_team_reasoners=tr_proportion,\n",
    "                    n_agents=32,\n",
    "                    init_network=G,\n",
    "                    games=my_game_mixture,#prisoners_dilemma,#my_game_mixture,\n",
    "                    probability_team_reasoning=1,\n",
    "                    utility_calculation=\"expected_utility\",\n",
    "                    learner = bayesian_gaussian_process(window=12, kappa=4, xi=0.0, alpha=.7)\n",
    "\n",
    "                )\n",
    "\n",
    "\n",
    "                # expected_utility\n",
    "                for t in tqdm.tqdm_notebook(range(30)):\n",
    "                    this_model.step()\n",
    "                model_list.append(this_model)\n",
    "                n+=1\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "pickle.dump(model_list, open('instep/random_partition_simulations_pickle.pkl', 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully_connected\n",
    "\n",
    "from simple_teams import model \n",
    "from simple_teams.learners import bayesian_gaussian_process \n",
    "\n",
    "\n",
    "model_list = []\n",
    "\n",
    "\n",
    "\n",
    "pd_share = [0.5]#np.linspace(0,1,9)#9\n",
    "tr_proportions = np.linspace(0,1,21)#11\n",
    "\n",
    "for this_pd_share in tqdm.tqdm_notebook(pd_share):\n",
    "    my_game_mixture = game_mixture([[prisoners_dilemma,this_pd_share],\n",
    "                                [hilo,1-this_pd_share]])\n",
    "    \n",
    "    for tr_proportion in tqdm.tqdm_notebook(tr_proportions):\n",
    "        \n",
    "        n=0\n",
    "        while n<3: #dirty hack to avoid untracked error\n",
    "#         for ix in range(0,3):\n",
    "    #         G = nx.powerlaw_cluster_graph(32,1,0.01)\n",
    "            try:\n",
    "                G = nx.complete_graph(32)\n",
    "\n",
    "                G = ensure_graph_connectivity(G)\n",
    "\n",
    "\n",
    "                this_model = model.team_reasoning_model(\n",
    "                    proportion_team_reasoners=tr_proportion,\n",
    "                    n_agents=32,\n",
    "                    init_network=G,\n",
    "                    games=my_game_mixture,#prisoners_dilemma,#my_game_mixture,\n",
    "                    probability_team_reasoning=1,\n",
    "                    utility_calculation=\"expected_utility\",\n",
    "                    learner = bayesian_gaussian_process(window=12, kappa=4, xi=0.0, alpha=.7)\n",
    "\n",
    "                )\n",
    "\n",
    "\n",
    "                # expected_utility\n",
    "                for t in tqdm.tqdm_notebook(range(30)):\n",
    "                    this_model.step()\n",
    "                model_list.append(this_model)\n",
    "                n+=1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            \n",
    "pickle.dump(model_list, open('instep/complete_graph_simulations_pickle.pkl', 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_teams import model \n",
    "from simple_teams.learners import bayesian_gaussian_process \n",
    "\n",
    "\n",
    "model_list = []\n",
    "\n",
    "\n",
    "\n",
    "pd_share = [0.5]#np.linspace(0,1,9)#9\n",
    "tr_proportions = np.linspace(0,1,21)#11\n",
    "\n",
    "for this_pd_share in tqdm.tqdm_notebook(pd_share):\n",
    "    my_game_mixture = game_mixture([[prisoners_dilemma,this_pd_share],\n",
    "                                [hilo,1-this_pd_share]])\n",
    "    \n",
    "    for tr_proportion in tqdm.tqdm_notebook(tr_proportions):\n",
    "        n=0\n",
    "        while n<3: #dirty hack to avoid untracked error\n",
    "\n",
    "            try:\n",
    "                G = nx.powerlaw_cluster_graph(32,1,0.01)\n",
    "                G = ensure_graph_connectivity(G)\n",
    "\n",
    "\n",
    "                this_model = model.team_reasoning_model(\n",
    "                    proportion_team_reasoners=tr_proportion,\n",
    "                    n_agents=32,\n",
    "                    init_network=G,\n",
    "                    games=my_game_mixture,#prisoners_dilemma,#my_game_mixture,\n",
    "                    probability_team_reasoning=1,\n",
    "                    utility_calculation=\"expected_utility\",\n",
    "                    learner = bayesian_gaussian_process(window=12, kappa=4, xi=0.0, alpha=.7)\n",
    "\n",
    "                )\n",
    "\n",
    "\n",
    "                # expected_utility\n",
    "                for t in tqdm.tqdm_notebook(range(30)):\n",
    "                    this_model.step()\n",
    "                model_list.append(this_model)\n",
    "                n+=1\n",
    "            except:\n",
    "                print('some error')\n",
    "                pass\n",
    "            \n",
    "pickle.dump(model_list, open('instep/powerlaw_cluster_simulations_pickle_2.pkl', 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_list_2 = pickle.load(open(r\"instep/random_partition_simulations_pickle.pkl\",\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_payoffs = []\n",
    "for ix,this_model in enumerate(model_list_2):\n",
    "    current_tr_proportion =this_model.proportion_team_reasoners\n",
    "    for agent in [this_model.schedule._agents[k] for k in this_model.schedule._agents.keys()]:\n",
    "#         print(agent.team_reasoner)\n",
    "        collected_payoffs.append((current_tr_proportion,\n",
    "                                  np.sum([x['target'] for x  in agent.my_learner.attempts]),\n",
    "                                 np.int(agent.team_reasoner)))\n",
    "        \n",
    "        \n",
    "collected_payoffs = np.array(collected_payoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygam import LinearGAM\n",
    "from pygam import ExpectileGAM\n",
    "\n",
    "X,y = collected_payoffs[:,0].reshape(-1,1), collected_payoffs[:,1].reshape(-1,1)\n",
    "\n",
    "gam = LinearGAM().gridsearch(X, y)\n",
    "# gam.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tr = collected_payoffs[:,2].astype(bool)#np.where(collected_payoffs[:,2]==1)\n",
    "\n",
    "X_tr, y_tr = collected_payoffs[w_tr,0].reshape(-1,1),collected_payoffs[w_tr,1].reshape(-1,1)\n",
    "\n",
    "gam_tr = LinearGAM().gridsearch(X_tr, y_tr)\n",
    "# gam_tr.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_nontr = ~collected_payoffs[:,2].astype(bool)#np.where(collected_payoffs[:,2]==1)\n",
    "# print(w_nontr)\n",
    "X_nontr, y_nontr = collected_payoffs[w_nontr,0].reshape(-1,1),collected_payoffs[w_nontr,1].reshape(-1,1)\n",
    "\n",
    "gam_nontr = LinearGAM().gridsearch(X_nontr, y_nontr)\n",
    "# gam_nontr.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 10))\n",
    "newcmap = cmocean.tools.crop_by_percent(cmocean.cm.haline, 30, which='both', N=None)\n",
    "\n",
    "plt.scatter(x=collected_payoffs[:,0]+np.random.rand(collected_payoffs.shape[0])*0.02,\n",
    "            y= collected_payoffs[:,1],\n",
    "            c=[['#1a2340','#f98400'][int(x)] for x in collected_payoffs[:,2]],\n",
    "           alpha=0.5\n",
    "           )\n",
    "\n",
    "# plt.scatter(x=collected_payoffs[w_tr,0]+np.random.rand(collected_payoffs[w_tr,0].shape[0])*0.02,\n",
    "#             y= collected_payoffs[w_tr,1],\n",
    "#            )\n",
    "\n",
    "# plt.plot(np.unique(X), gam.predict(np.unique(X)), color='black', linewidth=3)\n",
    "# conf = gam.confidence_intervals(np.unique(X), width=0.95, quantiles=None)\n",
    "# ax.fill_between(np.unique(X),conf[:,0],conf[:,1], \n",
    "#                  facecolor='black', alpha=0.2,interpolate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(np.unique(X_tr), gam_tr.predict(np.unique(X_tr)), color='#f98400', linewidth=3)\n",
    "conf = gam_tr.confidence_intervals(np.unique(X_tr), width=0.95, quantiles=None)\n",
    "ax.fill_between(np.unique(X_tr),conf[:,0],conf[:,1], \n",
    "                 facecolor='#f98400', alpha=0.4,interpolate=True)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(np.unique(X_nontr), gam_nontr.predict(np.unique(X_nontr)), color='#1a2340', linewidth=3)\n",
    "conf = gam_nontr.confidence_intervals(np.unique(X_nontr), width=0.95, quantiles=None)\n",
    "ax.fill_between(np.unique(X_nontr),conf[:,0],conf[:,1], \n",
    "                 facecolor='#1a2340', alpha=0.2,interpolate=True)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Proportion of Team-Reasoners\")\n",
    "plt.ylabel(\"Aggregated Payoffs\")\n",
    "# plt.show()\n",
    "\n",
    "# plt.savefig('instep/community_payoffs.png',dpi=300,bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_proportions = np.linspace(0,1,11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_df = model.datacollector.get_model_vars_dataframe()\n",
    "# agent_df = model.datacollector.get_agent_vars_dataframe()\n",
    "\n",
    "# agent_df = agent_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [.my_learner.my_distributions[].expect() for y in model.schedule._agents[20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# fig, ax =plt.subplots(figsize=(20,20))\n",
    "\n",
    "# step_to_plot = 2\n",
    "# plt.title(\n",
    "#     \"Probability of team-reasoning after \"\n",
    "#     + str(step_to_plot)\n",
    "#     + \" iterations\"\n",
    "# )\n",
    "# final_agent_df = agent_df[agent_df[\"Step\"] == step_to_plot]\n",
    "\n",
    "\n",
    "\n",
    "# G = graph_list[step_to_plot]\n",
    "# # positions = nx.kamada_kawai_layout(G)\n",
    "# # position_array = np.array([positions[x] for x in positions])\n",
    "# position_array = coords_list[step_to_plot]\n",
    "\n",
    "# trs = final_agent_df[\"team_reasoner\"]\n",
    "# nx.draw_networkx_edges(G, position_array, edge_color=\"grey\", alpha=0.5)\n",
    "\n",
    "\n",
    "\n",
    "# tr_scatter= plt.scatter(position_array[trs,0],\n",
    "#             position_array[trs,1], s= 100,#final_agent_df['gathered_utility'][trs] *.3,\n",
    "#             c=final_agent_df['probability_team_reasoning'][trs],\n",
    "#             cmap=cmocean.cm.thermal,\n",
    "#             vmin=0, vmax=1,alpha=1.\n",
    "            \n",
    "#            )\n",
    "\n",
    "# non_tr_scatter = plt.scatter(position_array[~trs,0],\n",
    "#             position_array[~trs,1],  s= 110\n",
    "#             , marker = '*',\n",
    "#            c=final_agent_df['probability_team_reasoning'][~trs],cmap=cmocean.cm.thermal,\n",
    "#             vmin=0, vmax=1\n",
    "           \n",
    "#            )\n",
    "\n",
    "# # divider = make_axes_locatable(ax)\n",
    "# # cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "   \n",
    "# cbar = plt.colorbar(tr_scatter,fraction=0.02, pad=0.0)\n",
    "# cbar.set_label('Probability of team-reasoning',fontsize=10)\n",
    "# cbar.ax.tick_params(labelsize=10)\n",
    "# ax.tick_params(labelsize=10)\n",
    "\n",
    "\n",
    "\n",
    "# plt.legend((tr_scatter,non_tr_scatter),\n",
    "#            ('Team-Reasoners','Non-Team-Reasoners'),\n",
    "#            scatterpoints=3,\n",
    "#            loc='upper right',\n",
    "#            ncol=1,\n",
    "#            fontsize=8)\n",
    "# ax = plt.gca()\n",
    "# leg = ax.get_legend()\n",
    "# leg.legendHandles[0].set_color('#fb9e40')\n",
    "# leg.legendHandles[1].set_color('#092f59')\n",
    "\n",
    "\n",
    "# plt.axis('equal')\n",
    "# plt.savefig('network_towards_the_beginning')#,dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "# plt.style.use(\"opinions.mplstyle\")\n",
    "\n",
    "to_plot = agent_df[agent_df[\"Step\"] == np.max(agent_df[\"Step\"])]\n",
    "\n",
    "# Plot the orbital period with horizontal boxes\n",
    "sns.boxplot(\n",
    "    x=\"team_reasoner\",\n",
    "    y=\"gathered_utility\",\n",
    "    data=to_plot,\n",
    "    whis=[0, 100],\n",
    "    width=0.6,\n",
    "    palette=[\"#1a2340\", \"#f2ad00\"],\n",
    ")\n",
    "\n",
    "# Add in points to show each observation\n",
    "sns.stripplot(\n",
    "    x=\"team_reasoner\",\n",
    "    y=\"gathered_utility\",\n",
    "    data=to_plot,\n",
    "    size=4,\n",
    "    color=\"black\",\n",
    "    linewidth=0,\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Team-reasoner\")\n",
    "plt.ylabel(\"Utility\")\n",
    "plt.title(\n",
    "    \"Achieved utility in PD after \" + str(np.max(agent_df[\"Step\"])) + \" iterations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pl\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmap = cmocean.cm.solar_r  # pl.cm.RdBu\n",
    "my_cmap = cmap(np.arange(cmap.N))\n",
    "alphas = np.linspace(0, 1, cmap.N)\n",
    "my_cmap[:, -1] = alphas\n",
    "my_tr_cmap = ListedColormap(my_cmap)\n",
    "\n",
    "cmap = cmocean.cm.deep  # pl.cm.RdBu\n",
    "my_cmap = cmap(np.arange(cmap.N))\n",
    "alphas = np.linspace(0, 1, cmap.N)\n",
    "my_cmap[:, -1] = alphas\n",
    "my_non_tr_cmap = ListedColormap(my_cmap)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "agents = [model.schedule._agents[x] for x in model.schedule._agents.keys()]\n",
    "# agents = agents[0:2]\n",
    "mappings = []\n",
    "for agent in tqdm.tqdm_notebook(agents):\n",
    "    collected_values = []\n",
    "    for ix, distribution in enumerate(agent.my_learner.distributions):\n",
    "\n",
    "        n = 500\n",
    "        x = np.linspace(0, 1, n)\n",
    "        alphas = distribution[0]\n",
    "        #         alphas = (alphas - np.min(alphas)) / (np.max(alphas) - np.min(alphas))\n",
    "        collected_values.append(alphas)\n",
    "    mappings.append(np.hstack([x.reshape(-1, 1) for x in collected_values]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_stack = np.mean(\n",
    "    np.stack(\n",
    "        [\n",
    "            mappings[x]\n",
    "            for x in np.where([agent.team_reasoner for agent in agents])[0]\n",
    "        ], axis=2\n",
    "    ),\n",
    "    axis=2,\n",
    ")\n",
    "\n",
    "non_tr_stack =np.mean(\n",
    "    np.stack(\n",
    "        [\n",
    "            mappings[x]\n",
    "            for x in np.where([not agent.team_reasoner for agent in agents])[0]\n",
    "        ], axis=2\n",
    "    ),\n",
    "    axis=2,\n",
    ")\n",
    "\n",
    "\n",
    "sns.heatmap(tr_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom = (non_tr_stack-tr_stack)/np.max(non_tr_stack-tr_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "# plt.imshow(tr_stack,cmap=my_tr_cmap,origin='upper', aspect='auto')\n",
    "# facecolor='#FF0000'\n",
    "plt.imshow(tr_stack,cmap=my_tr_cmap,origin='upper', aspect='auto')\n",
    "# plt.imshow(non_tr_stack,cmap=my_non_tr_cmap,origin='upper', alpha= dom, aspect='auto')\n",
    "plt.savefig('stream_plot_2_tr_70')#,dpi=600)\n",
    "\n",
    "# remem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.T-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "a = np.repeat(np.linspace(0,1,500),500).reshape(500,500)\n",
    "# a_ = a.T+a\n",
    "# ap = a_#(a_)/np.max(a_)\n",
    "\n",
    "plt.imshow(a,cmap=my_tr_cmap,origin='upper', aspect='auto',alpha=a-a.T/3)\n",
    "plt.imshow(a.T,cmap=my_non_tr_cmap,origin='upper', aspect='auto',alpha=a.T-a/3)\n",
    "\n",
    "plt.savefig('stream_plotlegend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_teams.generate_colormap import get_continuous_cmap\n",
    "\n",
    "\n",
    "hex_list = ['#4b1148','#4b1148','#d98773', '#ecefe4','#ecefe4','#68aa88','#151f44','#151f44', ]\n",
    "float_list=[0,.15,.35,.5,.51, .65,.85, 1]\n",
    "    \n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "sns.heatmap(tr_stack+non_tr_stack\n",
    "            ,cmap=get_continuous_cmap(hex_list, float_list=float_list),\n",
    "                cbar=True,center=0\n",
    ")\n",
    "ax.invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(np.max(np.stack([tr_stack,non_tr_stack],axis=2),axis=2))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "sns.heatmap(tr_stack-non_tr_stack\n",
    "            ,cmap=cmocean.cm.solar_r,\n",
    "                cbar=True,center=0\n",
    ")\n",
    "ax.invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x[0] for x in graph_listsb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mesa.batchrunner import BatchRunner,BatchRunnerMP\n",
    "from simple_teams.model import team_reasoning_model\n",
    "from simple_teams.learners import bayesian_beta_learner\n",
    "\n",
    "\n",
    "fixed_params = {\n",
    "    \"n_agents\": 200,\n",
    "    \"game\": prisoners_dilemma,\n",
    "    \"probability_team_reasoning\": 1.0,  # we need to take this out\n",
    "    \"utility_calculation\": \"expected_utility\",\n",
    "    # \"learner\": bayesian_beta_learner(prior={'type':'flat'})\n",
    "}\n",
    "\n",
    "#bayesian_beta_learner(prior={'type':'flat'})\n",
    "var_params = {\n",
    "        \"tr_threshold\": np.linspace(0.05,0.95,5),\n",
    "        \"proportion_team_reasoners\":np.linspace(0.05,0.95,5),\n",
    "        \"init_network\": [x[0] for x in graph_lists][0],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# batchrunner.BatchRunner(model_cls, variable_parameters=None, fixed_parameters=None,\n",
    "# iterations=1, max_steps=1000, model_reporters=None, agent_reporters=None, display_progress=True)\n",
    "batch_run = BatchRunner(\n",
    "team_reasoning_model,\n",
    "variable_parameters=var_params,\n",
    "fixed_parameters=fixed_params,\n",
    "iterations=3,\n",
    "max_steps=100,\n",
    "display_progress=True, \n",
    "   agent_reporters = {\"my_learner\":\"my_learner\"},\n",
    "\n",
    "# model_reporters = {\n",
    "#             \"proportion_team_reasoners\": lambda a: getattr(\n",
    "#                 a, \"proportion_team_reasoners\", None\n",
    "#             ),\n",
    "#         }\n",
    ")\n",
    "\n",
    "batch_run.run_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_data = batch_run.get_model_vars_dataframe()\n",
    "run_data = batch_run.get_agent_vars_dataframe()\n",
    "run_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this_run in np.unique(run_data['Run']):\n",
    "#     this_run_data = run_data[run_data['Run']==this_run]\n",
    "#     span = np.linspace(0, 1, 100)\n",
    "#     learned_distributions = []\n",
    "#     for this_learner in this_run_data['my_learner']:\n",
    "#         this_dist = this_learner.my_distributions[-1].pdf(span)\n",
    "#         # plt.plot(span,this_dist,alpha=0.03,c='#1a2340')\n",
    "#         learned_distributions.append(this_dist)\n",
    "    \n",
    "#     plt.plot(span,np.mean(np.vstack(learned_distributions),axis=0),\n",
    "#              alpha=.9,c='#B12806')\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parameter_grid_learners(data,row,column,figsize=(10,10)):\n",
    "    row_values = np.unique(data[row])\n",
    "    column_values = np.unique(data[column])\n",
    "    \n",
    "    fig = plt.figure(constrained_layout=True, figsize=figsize)\n",
    "    gs = fig.add_gridspec(nrows=len(row_values), ncols=len(column_values))\n",
    "\n",
    "    counter = 0\n",
    "    span = np.linspace(0, 1, 100)\n",
    "\n",
    "    for ix,x in enumerate(row_values):\n",
    "        for iy,y in enumerate(column_values):\n",
    "            this_data = data[(data[row]==x) & (data[column]==y)]\n",
    "            this_axis = fig.add_subplot(gs[ix,iy])\n",
    "            \n",
    "            for this_run in np.unique(this_data['Run']):\n",
    "                this_run_data = this_data[this_data['Run']==this_run]\n",
    "                learned_distributions = []\n",
    "                for this_learner in this_run_data['my_learner']:\n",
    "                    this_dist = this_learner.my_distributions[-1].pdf(span)\n",
    "                    # plt.plot(span,this_dist,alpha=0.03,c='#861111')\n",
    "                    learned_distributions.append(this_dist)\n",
    "                \n",
    "                \n",
    "                # print(row +' = ' + str(x) +'; '+ column +' = ' + str(y))\n",
    "                this_axis = plt.plot(span,np.mean(np.vstack(learned_distributions),axis=0),\n",
    "                alpha=.3,c='#6B1307')\n",
    "                # this_axis.suptitle(row +' = ' + str(x) +'; '+ column +' = ' + str(y))\n",
    "             \n",
    "            \n",
    "            counter+=1\n",
    "            \n",
    "plot_parameter_grid_learners(run_data,'tr_threshold','proportion_team_reasoners',figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_parameter_grid_learners(run_data,'tr_threshold',\n",
    "                             'proportion_team_reasoners',figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Johannes Notes:\n",
    "\n",
    "    * Discuss TR assymetry: Why can't nontrs learn?\n",
    "    * Brauchen wir überhaupt Dirichlet, wenn das Lernen nur TR vs non-TR ist?\n",
    "    \n",
    "Leyla notes:\n",
    "    * Eigene probability einbeziehen?\n",
    "    * Erkläre mir: Wie entscheiden wir aufgrund payoffs? "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "60ba6180c9982738441b2f5d01bb9584dbb06b5e8ab5f7a150e3e8961c339d19"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
